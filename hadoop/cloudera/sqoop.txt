/*** Sqoop commands to import data from mysql to HDFS */

sqoop list-databases --connect "jdbc:mysql://quickstart.cloudera:3306" --username retail_dba --password cloudera


sqoop list-tables --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username retail_dba --password cloudera

sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba --password cloudera \
--query "select count(1) from order_items"


sqoop import-all-tables \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--warehouse-dir=/user/cloudera/sqoop_import

-- import departments into HDFS
-- Basic import
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/cloudera/departments
  
  --import into Hive warehouse directory
  --copying into existing table ,u need to say 'append'
  -- and if less number of rows , just use one mapper
  --before this command , drop any rows in departments table
  -- in Hive and select from departments after this import
  
  sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --num-mappers 1 \
  --outdir java_files
  
  
-- Importing table with out primary key using multiple threads (split-by)
-- When using split-by, using indexed column is highly desired
-- If the column is not indexed then performance will be bad 
-- because of full table scan by each of the thread
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --split-by department_id \
  --outdir java_files

-- Getting delta (--where) will get u specific rows/updated rows
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --split-by department_id \
  --where "department_id > 7" \
  --outdir java_files
  
  --Incremental load
  
  sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --check-column "department_id" \
  --incremental append \
  --last-value 7 \
  --outdir java_files
  
  **incremental load always gives u the last values to save a job for u 
  **so u can pick up the furthur loading from where u left
  
  sqoop job --create sqoop_job \
  -- import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --check-column "department_id" \
  --incremental append \
  --last-value 7 \
  --outdir java_files
  
 
 /** import into Hive database */

sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--warehouse-dir=/user/hive/warehouse \
--hive-import \
--hive-overwrite \
--create-hive-table --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--outdir java_files


--Create hive table example
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --hive-home /user/hive/warehouse \
  --hive-import \
  --hive-table departments_test \
  --create-hive-table \
  --outdir java_files


/** specify the hive database that u want to import to */

sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--hive-import \
--hive-overwrite \
--create-hive-table --hive-database retail_edw \
--compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--outdir java_files

/** Append into hive database using check-column , incremental , last-value   , --append will append data to an existing dataset in HDFS , if not in place , subsequent imports gets only the new rows in HDFS and removes ols part-m-00* files, 
notice --hive-overwrite is not in place , if u are appending only new rows into Hive table , the hive-overwrite will 
remove the old part-m files and bring in only new part-m-00* files **/

sqoop import  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username retail_dba \                             --password cloudera --table departments --hive-import --check-column department_id \                                         --incremental append --last-value 7

/** Append into HDFS using check-column , incremental , last-value   , below Sqoop import creates same results regardless of of using --append or not , all new rows are brought into HDFS **/

sqoop import  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" --username retail_dba \                            --password cloudera --table departments --append --target-dir /user/cloudera/sqoop_import/departments \                      --check-column department_id --incremental append --last-value 7
