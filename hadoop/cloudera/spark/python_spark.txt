
//mostly derived my version from Durga Gadiraju hadoop videos and cloudra hadoop getting started tutorial
//Problem statement - Get Per order Total, Total Order Revenue per day and Total number of orders per day
//and calculate average revenue per day
//Orders Map - first element is the order id - notice the key is converted to integer
ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersMap = ordersRDD.map(lambda rec: (int(rec.split(",")[0]) , rec))

//orderItemsMap - 2nd element is the order_id 
orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
orderItemsMap = orderItemsRDD.map(lambda rec: (int(rec.split(",")[1]) , rec))
//since we r joining above 2 maps , both keys needs to the same , order_id's and
//should be of same datatypes
//join orders data onto orderitems data - always join smaller table onto a bigger table
//while joining the data
orderItemsjoinOrders = orderItemsMap.join(ordersMap)

//get the string 'order#,orderDate' from orderItems join for the key and order_item_subtota
//for the value, so we can add those values in next step to get the perOrderTotal
revenuePerOrderPerdate = orderItemsjoinOrders.map(lambda t: (str(t[0]) + "," + \
t[1][1].split(",")[1] , float(t[1][0].split(",")[4]))

//revenuePerOrderPerdate has the map of order#/orderdate , order_item_subtotal
//every individual order total , by adding all order item subtotal for an order
perOrderTotal = revenuePerOrderPerdate.reduceByKey(lambda total1 ,total2: total1 + total2)
for i in perOrderTotal.take(50):
 print(i)
 


//get the first element/key from the join , and then the 2nd element in the tuple (which again is a tuple rec[1])
//rec[1][1] needs split to give us the date - 2nd element after the split
ordersPerday = orderItemsjoinOrders.map(lambda rec: (str(rec[0]) + "," + rec[1][1].split(",")[1])).distinct()

//retrieve order # and order date from orderItemsJoinOrders and get the distinct 
//since each order/date combo can be in there for all the order items in a order
ordersPerday = orderItemsjoinOrders.map(lambda rec: (str(rec[0]) + "," + rec[1][1].split(",")[1])).distinct()

//since the order/date compound Key is distinct , just get the date from
//ordersPerday RDD and add a value 1 for each date entry
ordersPerdayCount = ordersPerday.map(lambda rec:  (rec.split(",")[1] , 1))

//the dates of the year will have a entry 1 for each order on a specific day,
//so sum up all the 1's to get no of orders on a day
//this yields us no of orders per day in the dataset
ordersPerdaySummedup = ordersPerdayCount.reduceByKey(lambda x,y: x+y)

ordersPerdaySummedup.count()
>>above yields result 364 - close to all days in a year


//Perdayrevenue
revenueAllOrdersPerdate = orderItemsjoinOrders.map(lambda t: (t[1][1].split(",")[1] , float(t[1][0].split(",")[4])))
//do the for loop for all of the RDD's that u create here to cross check
//cross verify
for rec in revenueAllOrdersPerdate:
 print(rec)
 
revenuePerday = revenueAllOrdersPerdate.reduceByKey(lambda t1,t2: t1 + t2)
(crosscheck in source database - select sum(order_item_subtotal) , order_date from orders o , order_items oi where o.order_id = oi.order_item_order_id group by order_date;)

//u can again join ordersPerday and Revenue Per day , to get to avgRevenuePerday !!!
//this RDD will save , date as key and the tuple(ordersPerday , RevenuePerday) as value
finalJoinRDD = ordersPerdaySummedup.join(revenuePerday)

//now arrive at avgRevenuePer day from above
avgRevenuePerDay = finalJoinRDD.map(lambda rec: (rec[0] , rec[1][1] / rec[1][0]))

//print this after sortByKey to see it from latest date
for rec in finalJoinRDD.sortByKey.collect():
 print(rec)
 #####All the above can be done use HiveContext in spark , just by using one sql
 ###Here is how
 
 from pyspark.sql import HiveContext
 sqlContext = HiveContext(sc)
 
 ###This needs to be set as an performance enhancing option
 ###The join or the collect/take from the join will fetch results , but 
 ### will take more time , if not set
 sqlContext.sql("set spark.sql.shuffle.partitions=10");
 
 joinDataFinal = sqlContext.sql("select o.order_date , count(distinct o.order_id) , sum(oi.order_item_subtotal) \
 from orders o join order_items oi on o.order_id = oi.order_item_id group by o.order_date order by o.order_date")
 
 for rec in joinDataFinal.take(50):
   print(rec)
   
 for rec in joinDataFinal.collect():
   print(rec)
   
 
 **** Using Spark SQL ****
 
 from pyspark.sql import SQLContext, Row
sqlContext = SQLContext(sc)
sqlContext.sql("set spark.sql.shuffle.partitions=10");

ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersMap = ordersRDD.map(lambda o: o.split(","))
orders = ordersMap.map(lambda o: Row(order_id=int(o[0]), order_date=o[1], \
order_customer_id=int(o[2]), order_status=o[3]))
ordersSchema = sqlContext.inferSchema(orders)
ordersSchema.registerTempTable("orders")

orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
orderItemsMap = orderItemsRDD.map(lambda oi: oi.split(","))
orderItems = orderItemsMap.map(lambda oi: Row(order_item_id=int(oi[0]), order_item_order_id=int(oi[1]), \
order_item_product_id=int(oi[2]), order_item_quantity=int(oi[3]), order_item_subtotal=float(oi[4]), \
order_item_product_price=float(oi[5])))
orderItemsSchema = sqlContext.inferSchema(orderItems)
orderItemsSchema.registerTempTable("order_items")

joinAggData = sqlContext.sql("select o.order_date, sum(oi.order_item_subtotal), \
count(distinct o.order_id) from orders o join order_items oi \
on o.order_id = oi.order_item_order_id \
group by o.order_date order by o.order_date")

for data in joinAggData.collect():
  print(data)


****************************Calculate aggregate statistics (avg,sum or highest) using spark ************
#Mostly from Dgadiraju's spark tutorials ,trying out my own version
# Sum

//now , there are these spark actions or transformations , that we can apply on RDD
//which will fetch the same results above just in a one step or two
//Lets see how.
///Get the total number of distinct orders from order_items
//need to get this from order_items , since only the processed ordersm must have made into order_items
orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

orderItemsMap = orderItemsRDD.map(lambda rec: float(rec.split(",")[4]))
//Total revenue of orders from order Items
totalRevenue = orderItemsMap.reduce(lambda acc,val: acc +val)

totalOrders = orderItemsRDD.map(lambda rec: rec.split(",")[1]).distinct().count()

avgRevenue = totalRevenue/totalOrders

#Get max priced product from products table
#There is one record which is messing up default , delimiters
#Clean it up (we will see how we can filter with out deleting the record later)
hadoop fs -get /user/cloudera/sqoop_import/products
#Delete the record with product_id 685
hadoop fs -put -f products/part* /user/cloudera/sqoop_import/products

//find out the max priced product from products table
productsRDD = sc.textFile("/user/cloudera/sqoop_import/products")
productsMap = productsRDD.map(lambda rec: float(rec.split(",")[4]))

highPricedItem = productsMap.reduce(lambda x , y: x if(x >= y) else y)

// find all canceled orders with order total more than $1000
//Filter out cancelled orders first , get order_item subtotal added per order from order items
//now join these 2 RDDs and filter again by revenue >= 1000
//joining after the filter and after the reduceByKey , the join is
//much more efficient


ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
ordersRDD.count()

ordersParsedRDD = ordersRDD.filter(lambda rec: rec.split(",")[3] == "CANCELED").map(lambda rec: (int(rec.split(",")[0]) , rec))

ordersParsedRDD.count() // check the diff in this count and the ordersRDD.count()

for i in ordersParsedRDD.take(10): print(i)
ordersItemsParsedRDD = orderItemsRDD.map(lambda rec: (int(rec.split(",")[1]) , float(rec.split(",")[4]) ) )

for i in ordersItemsParsedRDD.take(10): print(i)
orderItemsAgg = ordersItemsParsedRDD.reduceByKey(lambda x,y: x + y)

order1 = orderItemsAgg.lookup(1) // just testing if some order that u know in there can be looked up
orderItemsJoinOrders = orderItemsAgg.join(ordersParsedRDD)

for i in orderItemsJoinOrders.sortByKey().take(10): print(i)
for i in orderItemsJoinOrders.filter(lambda rec: rec[1][0] >= 1000 ).take(10):print(i)
orderItemsJoinOrders.filter(lambda rec: rec[1][0] >= 1000 ).count()
//resulting count in 139 as opposed to durga's 1728 or something

*****Sorting and Ranking *****
// Global sorting , just sort by the highest price in the products
productsRDD = sc.textFile("/user/cloudera/sqoop_import/products")
//get fourth field - product_price as key for Global sort on price
productsPriceMap = productsRDD.map(lambda line: (float(line.split(",")[4]), line))
for i in productsPriceMap.sortByKey().take(25): print(i)
//sort in descending order, sortByKey takes a optional parameter , default is true 
for i in productsPriceMap.sortByKey(False).take(25): print(i)

//again secondary sorting using , have price and product_id as the key
productsPriceMap = productsRDD.map(lambda line: ((float(line.split(",")[4]) , int(line.split(",")[0])), line))
for i in productsPriceMap.sortByKey(False).take(5): print(i)

// just show the record , ignoring the composite key , since the record contains the key
for i in productsPriceMap.sortByKey(False).map(lambda rec: rec[1]).take(5): print(i)


// sort by price per category
//group by category first and then go for sorting of the list (need to know basic python opertaions
//on a list )
productsMap = productsRDD.map(lambda line: (line.split(",")[1], line))

productsGroupBy = productsMap.groupByKey().
//now "sorted" python operation takes a optional 'key' parameter and 'reverse' parameter
for i in productsGroupBy.map(lambda rec: sorted(rec[1] , key = lambda k: float(k.split(",")[4]) , reverse=True)).take(25):
print(i)

 


